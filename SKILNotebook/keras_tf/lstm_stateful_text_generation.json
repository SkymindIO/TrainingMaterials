{"paragraphs":[{"text":"%md\n##Stateful LSTM for Text Generation Example\n\n**Predict next character from a complete sentence**\n\nTask:\nFine-tune the model to achieve better performance\n","user":"admin","dateUpdated":"2018-10-25T08:38:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Stateful LSTM for Text Generation Example</h2>\n<p><strong>Predict next character from a complete sentence</strong></p>\n<p>Task:\n<br  />Fine-tune the model to achieve better performance</p>\n"}]},"apps":[],"jobName":"paragraph_1540456658249_-1731846797","id":"20181025-083738_277445035","dateCreated":"2018-10-25T08:37:38+0000","dateStarted":"2018-10-25T08:38:28+0000","dateFinished":"2018-10-25T08:38:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:107"},{"title":"Python imports","text":"%pyspark\n\nimport numpy\nimport tensorflow\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n","user":"admin","dateUpdated":"2018-10-25T08:37:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-130715_402226220","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"title":"Python SKIL Context","text":"%pyspark\n\nimport skil\n\nskilContext = skil.SkilContext(sc)\n","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-130715_657142092","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%pyspark\n#fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\ntensorflow.set_random_seed(seed)","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-131051_1626730516","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%pyspark\n\nrawstr = '*This example trains a LSTM. Look for lab steps below. Uncomment to proceed.'#'*ABCDEFGHIJKLMNOPQRSTUVWXYZ'\ninputstr = rawstr.lower()\n\n#to eliminate redundant character\nuniquechars = sorted(list(set(inputstr)))\n\n#create mapping for characters\n#look up table->character : index\nlut = dict((c, i) for i, c in enumerate(uniquechars))\n#reverse look up table->index : character\nrut = dict((i, c) for i, c in enumerate(uniquechars))\n\nepoch = 1\n","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-131219_1265184387","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%pyspark\n\n'''\nStateful LSTM CheckOutput\n'''\ndef callBack():\n\n    prediction = ''\n    charnow = inputstr[0]\n\n    for i in range(1, len(inputstr)):\n        buffer = numpy.zeros((1, 1, len(uniquechars)), dtype = numpy.bool)\n        buffer[0, 0, lut[charnow]] = 1\n        predictClass = model.predict_classes(buffer)\n        charnow = rut[predictClass[0]]\n        prediction = prediction + charnow\n    print('Prediction:', prediction)\n\n    model.reset_states()\n","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-131333_1053773651","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%pyspark\n\n#input data dimension = (data samples, time steps, features)\ninputdata = numpy.zeros((len(inputstr), 1, len(uniquechars)), dtype = numpy.bool)\n#output data = (data samples, features)\nlabels = []\n\nprint('One to one mapping')\nfor i in range(0, len(inputstr) - 1):\n    inputdata[i, 0, lut[inputstr[i]]] = 1\n    labels.append(lut[inputstr[i + 1]])\n    #print(inputstr[i], '->', inputstr[i + 1])\n\n#next character of last one pointing to the [0]\nlabels.append(lut[inputstr[0]])\n\n#last character of input\nlastindex = len(inputstr) - 1;\ninputdata[lastindex, 0, lut[inputstr[lastindex]]] = 1\n\n#create one hot labels for classification\nonehotlabels = np_utils.to_categorical(labels)\n\n","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"One to one mapping\n"}]},"apps":[],"jobName":"paragraph_1540456354242_480229516","id":"20181004-131418_233554814","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:33:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%pyspark\n\nbatch_size = 1\n\n#input shape for first layer is (batch size, timesteps, features)\n#output in this case is number of unique characters\n\nmodel = Sequential()\nmodel.add(LSTM(units = 100, batch_input_shape=(batch_size, inputdata.shape[1], inputdata.shape[2]), stateful=True, return_sequences = True))\nmodel.add(LSTM(units = 50))\nmodel.add(Dense(units = onehotlabels.shape[1], activation = 'softmax'))\nmodel.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n\nfor i in range(300):\n    #keep the state of the network\n    model.fit(inputdata, onehotlabels, epochs = epoch, batch_size = batch_size, verbose = 2, shuffle = False)\n    model.reset_states()\n    \n    if((i % 10) == 0):\n        print('Training #%d' % i)\n        callBack()\n\n\n#final prediction string\ncallBack()","user":"admin","dateUpdated":"2018-10-25T08:38:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Epoch 1/1\n - 2s - loss: 3.1264 - acc: 0.1184\nTraining #0\n('Prediction:', '                                                                           ')\nEpoch 1/1\n - 0s - loss: 3.0152 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.9222 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.9013 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8890 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8801 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8731 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8667 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8556 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8391 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.8197 - acc: 0.1579\nTraining #10\n('Prediction:', '             t t tt tttt tt t t tt t t                                tt t ')\nEpoch 1/1\n - 0s - loss: 2.8029 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.7694 - acc: 0.1447\nEpoch 1/1\n - 0s - loss: 2.7919 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.7580 - acc: 0.1447\nEpoch 1/1\n - 0s - loss: 2.8083 - acc: 0.1447\nEpoch 1/1\n - 0s - loss: 2.7290 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.7251 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.7204 - acc: 0.1447\nEpoch 1/1\n - 0s - loss: 2.6987 - acc: 0.1053\nEpoch 1/1\n - 0s - loss: 2.6467 - acc: 0.1316\nTraining #20\n('Prediction:', '                eeeeeeeeeeeeeeeetccccccccccccccccccchhhhhhhiiii            ')\nEpoch 1/1\n - 0s - loss: 2.6192 - acc: 0.1184\nEpoch 1/1\n - 0s - loss: 2.6267 - acc: 0.1316\nEpoch 1/1\n - 0s - loss: 2.6534 - acc: 0.1184\nEpoch 1/1\n - 0s - loss: 2.5768 - acc: 0.1316\nEpoch 1/1\n - 0s - loss: 2.5617 - acc: 0.1316\nEpoch 1/1\n - 0s - loss: 2.5775 - acc: 0.1447\nEpoch 1/1\n - 0s - loss: 2.5327 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.4782 - acc: 0.1579\nEpoch 1/1\n - 0s - loss: 2.5108 - acc: 0.1711\nEpoch 1/1\n - 0s - loss: 2.4592 - acc: 0.1711\nTraining #30\n('Prediction:', 's s                           e   ooooooooooooooeeeeecchhhhiiiiis   eeeeeee')\nEpoch 1/1\n - 0s - loss: 2.4743 - acc: 0.1711\nEpoch 1/1\n - 0s - loss: 2.7738 - acc: 0.1316\nEpoch 1/1\n - 0s - loss: 2.4473 - acc: 0.1842\nEpoch 1/1\n - 0s - loss: 2.4458 - acc: 0.1842\nEpoch 1/1\n - 0s - loss: 2.4356 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.5392 - acc: 0.1711\nEpoch 1/1\n - 0s - loss: 2.6435 - acc: 0.2237\nEpoch 1/1\n - 0s - loss: 2.4190 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.3741 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.4982 - acc: 0.1842\nTraining #40\n('Prediction:', 'this eeeeee  aaaaaa      l l loooool l l l l                   ttttooetteee')\nEpoch 1/1\n - 0s - loss: 2.4215 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.3106 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.2396 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.1817 - acc: 0.2632\nEpoch 1/1\n - 0s - loss: 2.1522 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.1248 - acc: 0.2632\nEpoch 1/1\n - 0s - loss: 2.2852 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 2.2772 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.0867 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.1022 - acc: 0.2632\nTraining #50\n('Prediction:', 'this eeeee  taiiii  l l lokllllll l b beeebel..nnnnmoo  ooeeeeee***********')\nEpoch 1/1\n - 0s - loss: 2.2679 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 2.7372 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.6299 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.4587 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.1926 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.1895 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.0261 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 1.9821 - acc: 0.2500\nEpoch 1/1\n - 0s - loss: 2.1087 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.1862 - acc: 0.2237\nTraining #60\n('Prediction:', 'tth  eeeeeee   taiii a allll loooool l l l l ooooot t ttooeeeeeee****eeeeee')\nEpoch 1/1\n - 0s - loss: 2.0603 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.2713 - acc: 0.2237\nEpoch 1/1\n - 0s - loss: 2.2874 - acc: 0.2105\nEpoch 1/1\n - 0s - loss: 2.1928 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 2.0668 - acc: 0.2368\nEpoch 1/1\n - 0s - loss: 1.9485 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 1.9337 - acc: 0.3289\nEpoch 1/1\n - 0s - loss: 1.9614 - acc: 0.3158\nEpoch 1/1\n - 0s - loss: 2.4548 - acc: 0.2237\nEpoch 1/1\n - 0s - loss: 2.3145 - acc: 0.2500\nTraining #70\n('Prediction:', 'tth eeeeeeeeeeeeee   tt t  l l looooo l l          t t tooooeeeeeeepececece')\nEpoch 1/1\n - 0s - loss: 2.2274 - acc: 0.2500\nEpoch 1/1\n - 0s - loss: 2.1311 - acc: 0.2237\nEpoch 1/1\n - 0s - loss: 2.1975 - acc: 0.3289\nEpoch 1/1\n - 0s - loss: 2.1280 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 2.2494 - acc: 0.3289\nEpoch 1/1\n - 0s - loss: 2.1310 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.0701 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 2.1453 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.2910 - acc: 0.3158\nEpoch 1/1\n - 0s - loss: 2.1282 - acc: 0.2368\nTraining #80\n('Prediction:', 'thhs exapplettinn s s lmm. l l lllll llllooooooooooooonoeeeeeeeeeeeeeeeeeee')\nEpoch 1/1\n - 0s - loss: 2.0173 - acc: 0.2895\nEpoch 1/1\n - 0s - loss: 1.9761 - acc: 0.3158\nEpoch 1/1\n - 0s - loss: 1.9204 - acc: 0.3158\nEpoch 1/1\n - 0s - loss: 2.3589 - acc: 0.2763\nEpoch 1/1\n - 0s - loss: 2.3819 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.5055 - acc: 0.1974\nEpoch 1/1\n - 0s - loss: 2.3427 - acc: 0.2632\nEpoch 1/1\n - 0s - loss: 2.2476 - acc: 0.2237\nEpoch 1/1\n - 0s - loss: 2.0932 - acc: 0.3289\nEpoch 1/1\n - 0s - loss: 2.1259 - acc: 0.3158\nTraining #90\n('Prediction:', 'thi s eaeee tttiis  l loooooooooo ooooooooooeeeppppppp t tr nnnn nn nn lroo')\nEpoch 1/1\n - 0s - loss: 2.2645 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.0638 - acc: 0.2632\nEpoch 1/1\n - 0s - loss: 2.2977 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.0731 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 2.0501 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 1.9333 - acc: 0.3421\nEpoch 1/1\n - 0s - loss: 1.9074 - acc: 0.3553\nEpoch 1/1\n - 0s - loss: 1.8063 - acc: 0.3816\nEpoch 1/1\n - 0s - loss: 1.7570 - acc: 0.4211\nEpoch 1/1\n - 0s - loss: 1.6613 - acc: 0.4737\nTraining #100\n('Prediction:', 'this exaeple taii i  l l. looooooo s s s leleplep..nnnnn ceecece.u....... .')\nEpoch 1/1\n - 0s - loss: 1.6206 - acc: 0.5132\nEpoch 1/1\n - 0s - loss: 1.5849 - acc: 0.5263\nEpoch 1/1\n - 0s - loss: 1.5881 - acc: 0.5132\nEpoch 1/1\n - 0s - loss: 1.6049 - acc: 0.4868\nEpoch 1/1\n - 0s - loss: 1.4841 - acc: 0.5263\nEpoch 1/1\n - 0s - loss: 1.5782 - acc: 0.4605\nEpoch 1/1\n - 0s - loss: 1.7469 - acc: 0.4079\nEpoch 1/1\n - 0s - loss: 1.5068 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.4336 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.4271 - acc: 0.6447\nTraining #110\n('Prediction:', 'this exameletrains a ls ll looor lalal llll llllofofoofooofois st st steest')\nEpoch 1/1\n - 0s - loss: 1.5643 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.5693 - acc: 0.5526\nEpoch 1/1\n - 0s - loss: 1.4693 - acc: 0.4868\nEpoch 1/1\n - 0s - loss: 1.4280 - acc: 0.4868\nEpoch 1/1\n - 0s - loss: 2.5611 - acc: 0.3026\nEpoch 1/1\n - 0s - loss: 1.6778 - acc: 0.4605\nEpoch 1/1\n - 0s - loss: 1.4365 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.3679 - acc: 0.5526\nEpoch 1/1\n - 0s - loss: 1.9034 - acc: 0.3553\nEpoch 1/1\n - 0s - loss: 1.7323 - acc: 0.4211\nTraining #120\n('Prediction:', 'this examle trains a lstm. looooor aast ls s st s ls bpppnpppproooopppppppp')\nEpoch 1/1\n - 0s - loss: 1.6998 - acc: 0.5000\nEpoch 1/1\n - 0s - loss: 1.5269 - acc: 0.5132\nEpoch 1/1\n - 0s - loss: 1.4126 - acc: 0.6053\nEpoch 1/1\n - 0s - loss: 1.4136 - acc: 0.5132\nEpoch 1/1\n - 0s - loss: 1.5458 - acc: 0.5658\nEpoch 1/1\n - 0s - loss: 1.5407 - acc: 0.5263\nEpoch 1/1\n - 0s - loss: 1.3546 - acc: 0.5789\nEpoch 1/1\n - 0s - loss: 1.3477 - acc: 0.5921\nEpoch 1/1\n - 0s - loss: 1.5065 - acc: 0.5263\nEpoch 1/1\n - 0s - loss: 1.4647 - acc: 0.5000\nTraining #130\n('Prediction:', 'this example trais aleeloo. lo. nt t t teeco.o.ocoocoo.o...................')\nEpoch 1/1\n - 0s - loss: 1.7749 - acc: 0.4211\nEpoch 1/1\n - 0s - loss: 1.5542 - acc: 0.4868\nEpoch 1/1\n - 0s - loss: 1.4731 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.3498 - acc: 0.6053\nEpoch 1/1\n - 0s - loss: 1.2520 - acc: 0.6184\nEpoch 1/1\n - 0s - loss: 1.8912 - acc: 0.4079\nEpoch 1/1\n - 0s - loss: 1.3629 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 1.6100 - acc: 0.5658\nEpoch 1/1\n - 0s - loss: 1.5389 - acc: 0.4868\nEpoch 1/1\n - 0s - loss: 1.2995 - acc: 0.6447\nTraining #140\n('Prediction:', 'this example trains a lstm. loooor a ls teow. . nnt nt ppropro.....e...e...')\nEpoch 1/1\n - 0s - loss: 1.2371 - acc: 0.6447\nEpoch 1/1\n - 0s - loss: 1.2216 - acc: 0.6711\nEpoch 1/1\n - 0s - loss: 1.1271 - acc: 0.6974\nEpoch 1/1\n - 0s - loss: 1.4087 - acc: 0.5921\nEpoch 1/1\n - 0s - loss: 1.2880 - acc: 0.5921\nEpoch 1/1\n - 0s - loss: 1.5486 - acc: 0.5263\nEpoch 1/1\n - 0s - loss: 1.1616 - acc: 0.6579\nEpoch 1/1\n - 0s - loss: 1.3111 - acc: 0.6316\nEpoch 1/1\n - 0s - loss: 1.0368 - acc: 0.7105\nEpoch 1/1\n - 0s - loss: 0.9820 - acc: 0.7500\nTraining #150\n('Prediction:', 'this example trais a lstm. looooor laoor aas abes loooookoaa laeestew. loom')\nEpoch 1/1\n - 0s - loss: 1.2067 - acc: 0.6579\nEpoch 1/1\n - 0s - loss: 1.1620 - acc: 0.6447\nEpoch 1/1\n - 0s - loss: 1.0785 - acc: 0.6842\nEpoch 1/1\n - 0s - loss: 1.0786 - acc: 0.7105\nEpoch 1/1\n - 0s - loss: 0.8949 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.9483 - acc: 0.7237\nEpoch 1/1\n - 0s - loss: 0.8381 - acc: 0.7368\nEpoch 1/1\n - 0s - loss: 0.8181 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.9880 - acc: 0.7237\nEpoch 1/1\n - 0s - loss: 1.3795 - acc: 0.5658\nTraining #160\n('Prediction:', 'this example trais a lst lst looor aastes stes s stes ls loooor aaor astaes')\nEpoch 1/1\n - 0s - loss: 1.1038 - acc: 0.6316\nEpoch 1/1\n - 0s - loss: 1.0070 - acc: 0.7763\nEpoch 1/1\n - 0s - loss: 1.0290 - acc: 0.6579\nEpoch 1/1\n - 0s - loss: 1.0540 - acc: 0.7237\nEpoch 1/1\n - 0s - loss: 0.8000 - acc: 0.8026\nEpoch 1/1\n - 0s - loss: 0.8133 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.9563 - acc: 0.7105\nEpoch 1/1\n - 0s - loss: 0.8219 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.7866 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.7166 - acc: 0.8158\nTraining #170\n('Prediction:', 'this example trains a ls t. loo ook for lor la beelow. eelow. unt t t te to')\nEpoch 1/1\n - 0s - loss: 0.9980 - acc: 0.6974\nEpoch 1/1\n - 0s - loss: 0.6995 - acc: 0.7763\nEpoch 1/1\n - 0s - loss: 0.6601 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.7338 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.6338 - acc: 0.8421\nEpoch 1/1\n - 0s - loss: 0.6053 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 1.1090 - acc: 0.6711\nEpoch 1/1\n - 0s - loss: 0.9510 - acc: 0.7632\nEpoch 1/1\n - 0s - loss: 0.6880 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.7332 - acc: 0.7500\nTraining #180\n('Prediction:', 'this example trains a lstm. look for lab beelow. unw. uont. ulo.**.**.**.*.')\nEpoch 1/1\n - 0s - loss: 0.6798 - acc: 0.7632\nEpoch 1/1\n - 0s - loss: 0.7473 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.5724 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 0.5417 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.4721 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.5156 - acc: 0.8684\nEpoch 1/1\n - 0s - loss: 0.8384 - acc: 0.7632\nEpoch 1/1\n - 0s - loss: 0.6169 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 1.2588 - acc: 0.5395\nEpoch 1/1\n - 0s - loss: 0.8366 - acc: 0.8026\nTraining #190\n('Prediction:', 'this example ttrins t lst. look for lab els steow. unt mento ceoco ceed.**e')\nEpoch 1/1\n - 0s - loss: 0.5886 - acc: 0.8421\nEpoch 1/1\n - 0s - loss: 0.5389 - acc: 0.8684\nEpoch 1/1\n - 0s - loss: 0.6491 - acc: 0.8289\nEpoch 1/1\n - 0s - loss: 0.4187 - acc: 0.9211\nEpoch 1/1\n - 0s - loss: 0.6019 - acc: 0.8289\nEpoch 1/1\n - 0s - loss: 0.4862 - acc: 0.8553\nEpoch 1/1\n - 0s - loss: 0.5301 - acc: 0.8684\nEpoch 1/1\n - 0s - loss: 1.0337 - acc: 0.6842\nEpoch 1/1\n - 0s - loss: 0.4803 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 0.3896 - acc: 0.9474\nTraining #200\n('Prediction:', 'this example raains a a lloook foomme.ele..e..**.*.*e.*. .**. .**o .**o o *')\nEpoch 1/1\n - 0s - loss: 0.6462 - acc: 0.8421\nEpoch 1/1\n - 0s - loss: 0.3770 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.7030 - acc: 0.8289\nEpoch 1/1\n - 0s - loss: 0.4148 - acc: 0.9211\nEpoch 1/1\n - 0s - loss: 0.4331 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.3428 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.5417 - acc: 0.8684\nEpoch 1/1\n - 0s - loss: 0.4923 - acc: 0.8684\nEpoch 1/1\n - 0s - loss: 0.3483 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.3298 - acc: 0.9342\nTraining #210\n('Prediction:', 'this example trains a lstm. loook  lor lab stelow. uncommeento proceed.****')\nEpoch 1/1\n - 0s - loss: 0.2622 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.2425 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.4738 - acc: 0.8553\nEpoch 1/1\n - 0s - loss: 0.4656 - acc: 0.8421\nEpoch 1/1\n - 0s - loss: 0.3117 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.3729 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 0.7231 - acc: 0.7500\nEpoch 1/1\n - 0s - loss: 0.5955 - acc: 0.8026\nEpoch 1/1\n - 0s - loss: 0.5986 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.3688 - acc: 0.9079\nTraining #220\n('Prediction:', 'this example trains a lstm. look for lab stebelow. uncomment toment t ud.**')\nEpoch 1/1\n - 0s - loss: 0.4281 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.3342 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.3141 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.2593 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.3058 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.3238 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.2959 - acc: 0.9079\nEpoch 1/1\n - 0s - loss: 0.2971 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.2847 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.2959 - acc: 0.9211\nTraining #230\n('Prediction:', 'this example trains a lstm. lok fr locmment. t.**ed.********.***.***.***.**')\nEpoch 1/1\n - 0s - loss: 0.2137 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.1497 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.2560 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.4616 - acc: 0.8421\nEpoch 1/1\n - 0s - loss: 0.2649 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.3774 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.2675 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.2607 - acc: 0.9342\nEpoch 1/1\n - 0s - loss: 0.1582 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.3147 - acc: 0.9079\nTraining #240\n('Prediction:', 'this example trains a lstm. look for lab lab steps below. uncoment t to pro')\nEpoch 1/1\n - 0s - loss: 0.1623 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.8277 - acc: 0.8158\nEpoch 1/1\n - 0s - loss: 0.9793 - acc: 0.6842\nEpoch 1/1\n - 0s - loss: 0.6874 - acc: 0.7895\nEpoch 1/1\n - 0s - loss: 0.2069 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.2204 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.1246 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.1184 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0740 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.1146 - acc: 1.0000\nTraining #250\n('Prediction:', 'this example trains a lstm. loow. lrommmnt. to procomeced.*****************')\nEpoch 1/1\n - 0s - loss: 0.2114 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.1587 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.1337 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.1078 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.4275 - acc: 0.8816\nEpoch 1/1\n - 0s - loss: 0.1340 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.1278 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.0761 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0935 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0785 - acc: 1.0000\nTraining #260\n('Prediction:', 'this example trains a lstm. look for lab steps below. uncomment to proceed.')\nEpoch 1/1\n - 0s - loss: 0.0644 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0593 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.2113 - acc: 0.9474\nEpoch 1/1\n - 0s - loss: 0.1165 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.0585 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0397 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0849 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.3561 - acc: 0.8553\nEpoch 1/1\n - 0s - loss: 0.1238 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.0846 - acc: 1.0000\nTraining #270\n('Prediction:', 'this example trains a lstm. look for laab steps below. uncomment promenteed')\nEpoch 1/1\n - 0s - loss: 0.0492 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.1701 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.0619 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.1348 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0977 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.0899 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.0985 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.3133 - acc: 0.8947\nEpoch 1/1\n - 0s - loss: 0.1105 - acc: 0.9737\nEpoch 1/1\n - 0s - loss: 0.0624 - acc: 0.9868\nTraining #280\n('Prediction:', 'this example trains a lstm. look for lab steps below. uncomment to proceed.')\nEpoch 1/1\n - 0s - loss: 0.0518 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0791 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0486 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0624 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0257 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0313 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0297 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0601 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0298 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0212 - acc: 1.0000\nTraining #290\n('Prediction:', 'this example trains a lstm.. look for lab steps below. uncomment to proceed')\nEpoch 1/1\n - 0s - loss: 0.0574 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0294 - acc: 0.9868\nEpoch 1/1\n - 0s - loss: 0.0170 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0450 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.0184 - acc: 1.0000\nEpoch 1/1\n - 0s - loss: 0.1531 - acc: 0.9605\nEpoch 1/1\n - 0s - loss: 0.6305 - acc: 0.8289\nEpoch 1/1\n - 0s - loss: 0.8695 - acc: 0.7632\nEpoch 1/1\n - 0s - loss: 0.3289 - acc: 0.8684\n('Prediction:', 'this example trains a lstm. look for lab steps below. uncommentomment to pr')\n"}]},"apps":[],"jobName":"paragraph_1540456354243_479844767","id":"20181004-131526_2038277892","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:34+0000","dateFinished":"2018-10-25T08:35:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"title":"Python model saving.","text":"%pyspark\n\nmodel_id = skilContext.addModelToExperiment(z, model, \"Stateful LSTM for Sentence Prediction\")\n#skilContext.addEvaluationToModel(z, model_id, model, inputdata, onehotlabels, \"Stateful LSTM \" + str(epochs) + \" epochs\")\n","user":"admin","dateUpdated":"2018-10-25T08:38:53+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540456354243_479844767","id":"20181004-130715_372883443","dateCreated":"2018-10-25T08:32:34+0000","dateStarted":"2018-10-25T08:33:35+0000","dateFinished":"2018-10-25T08:35:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"%pyspark\n","user":"admin","dateUpdated":"2018-10-25T08:33:34+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540456354243_479844767","id":"20181004-130715_1258137860","dateCreated":"2018-10-25T08:32:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"}],"name":"/statefulLSTM","id":"2DTV2FWU8","angularObjects":{"2DUEGVJHV:existing_process":[],"2DUV6FQ57:existing_process":[],"2DUY5ZPD1:existing_process":[],"2DUEH9MQ1:existing_process":[],"2DVSBVTRW:existing_process":[],"2DTQQ28BY:existing_process":[],"2DVH2TXYZ:existing_process":[],"2DU7N5KJK:existing_process":[],"2DV4BQJY9:existing_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}