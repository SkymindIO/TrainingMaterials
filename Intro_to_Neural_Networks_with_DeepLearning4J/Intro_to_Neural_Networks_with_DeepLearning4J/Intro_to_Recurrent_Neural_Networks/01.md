# Introduction to Recurrent Neural Networks 

# Table of Contents

1. Fundamentals of Machine Learning
2. Something else

<div style="page-break-after: always;"></div>

----------------------
# What is a Recurrent Neural Network?

* FeedForward Network with hidden state
* Hidden state with own internal dynamics
* Information can be stored in "hidden state" for a long time


-------------------
<div style="page-break-after: always;"></div>

# Benefit of RNN

* Instead of pre-configured window of time steps
* Flexible state information for flexible length events

-------------------
<div style="page-break-after: always;"></div>

# Hidden Markov models and speech recognition

Use for speech recognition in the 70's not neural net

Not sure I should mention them

-------------------
<div style="page-break-after: always;"></div>

# Benefits of RNN's

* Distributed Hidden State
* Several different units can be active at the same time
* Can remember several different things
* Non-linear can update hidden state in complicated ways
* Hinton quote
"With enough neurons and enough time, a recurring neuron network can compute anything that can be computed by your computer." coursera course

-------------------
<div style="page-break-after: always;"></div>

# More Benefits What can RNN's model, what behavior can they exhibit

* Oscillations, motor control, walking robot
* Settle to point attractors 

-------------------
<div style="page-break-after: always;"></div>

# Challenges of RNN's

* Comp;exity makes them hard to train


-------------------
<div style="page-break-after: always;"></div>

# Training

* Back Propogation Through time
* think of it as feed forward network with constrained weights. 

Think of RNN as discrete time steps and (see Hinton Coursera)

-------------------
<div style="page-break-after: always;"></div>




# Training Goal, one sequence to another Sequence

* When modeling Sequential data we often want to turn one sequence into another sequence

* A phrase in english to a phrase in Spanish

* Sequence of audio ad convert into word identitites

-------------------
<div style="page-break-after: always;"></div>

# Training Goal

* Next timestep of current sequence

---------
<div style="page-break-after: always;"></div>


# Non Sequence data as Sequence data

* Pixels in an image , or Grid of pixels applied to next Grid
* works quite well, feels less natural

---------
<div style="page-break-after: always;"></div>

# Four ways to train an RNN

* LSTM 
 Make the RNN out of little modules that are designed to remember values for a long time. 
 
 * Use Better Optimizer HEssian Free
 
 That can deal with very small gradients. 
 (Martens & Sutskever 2011)
 
 * Echo State Networks
 
 This sound cool, oscilators that reverberate for a while. 

Not sure if this makes sense for this course

---------
<div style="page-break-after: always;"></div>

# LSTM in depth

Consider the current state as short term memory, then we want to find a way to make this long term

Add modules that allow information to be gated in, and information to be gated out when needed. 

In between information being gated in and gated out the gate is closed allowing it to remember the gated state. 

->Gate open->Information stored -> Gate open information forgot->

Repeat. 


---------
<div style="page-break-after: always;"></div>

# LSTM in depth

* Very succesful recognizing handwriting


---------
<div style="page-break-after: always;"></div>

# LSTM in depth

* Information gets into the cell whenever a logistic write gate is on

The rest of the recurrent network determines the state of that write gate, and when the rest of the recurrent network wants information to be stored, it turns the write gate on, and whatever the current input from the rest of the net to the memory cell is, gets stored in the memory cell.

-plagiarized Hinton-

The information stays in the memory cell so long as its keep gate is on. So again, the rest of the system is determining the state of a logistic keep gate, and if it keeps it on, then the information will stay there.

And finally, the information gets read from the memory cell so that it then goes off to the rest of the recurrent neural network and influences future states and it's read by turning on a read gate, Which again is a logistic unit controlled by the rest of the neural network. 

The memory cell actually stores an analog value, so we can think of it as a linear neuron that has an analog value and keeps writing that value to itself at each time step by a weight of one, so the information just stays there.

Grab and create a version of his discussion about keep gates, Lecture 7 Final video. 

---------
<div style="page-break-after: always;"></div>

# Cursive handwriting recogntion

Input is squence of pen coordinates as text is written

Output is sequence of characters

Graves & Schmidhuber (2009)

If timing of pen is not known, the sequence of small images as input works


---------
<div style="page-break-after: always;"></div>

# For Character Recognition

Ask what the net knows...
words? Yes
Days ? 

brackets, quotes, etc


See Lecture 8 3 Hinton


---------
<div style="page-break-after: always;"></div>

# RNN's require much less training data than others on NLP stuff. 


---------
<div style="page-break-after: always;"></div>


# A Diagram

![alt text](../resources/venn.png)





---------
<div style="page-break-after: always;"></div>

# Machine Learning Compared to Data Science/Mining

---------
<div style="page-break-after: always;"></div>
