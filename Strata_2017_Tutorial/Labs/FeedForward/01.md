# Building a Feed Forward Neural Net With DeepLearning4J

-----------------
<div style="page-break-after: always;"></div>

# What is a Feed Forward Neural Network

* A Neural network with X numbers of fully connected Layers 


# Feed Forward Neural Net:

![alt text](../resources/two_layer.png)

# Lab Introduction

* Feed Forward Neural Network with 2 hidden layers
* Learning to Recognize and Classify Images
  * 28 * 28 Grayscale

# Image examples

![alt text](../resources/18.png) ![alt text](../resources/19.png) ![alt text](../resources/23.png)

# Exploring the dataset

* Data stored in 

```
resources/mnist_png
```

* Pre-split into training and testing
* stored in labelled directories




-------------------
<div style="page-break-after: always;"></div>

# Lab Step 1: 

Open Intellij and navigate to the Labs project. 

The project contains a Labs directory with stub programs for you to complete, a demos directory with demos, and a solutions directory with solutions for the lab.


```
src/main/java/ai.skymind.training/labs
```

# Lab Step 2:

Set some image related variables for later use.

The Mnist images are 28 * 28 grayscale. 


```
int height = 28;
int width = 28;
int channels = 1;
```

Add three lines to the code stub specifying height width and channels. 

# Lab Step 3:


Set an int for Random Number Seed. Same seed with different runs allow for consistent results. 


```
int rngseed = 123;
Random randNumGen = new Random(rngseed);
```

Intellij will highlight the code in red, until the needed import statement is added  "import java.util.Random;"



# Lab Step 4:

Set Batchsize. Batchsize determines how many records to train on before adjusting weights. In this case we are classifying to 10 possible classes, our batchsize should be large enough and our data randomized to include records from each class in the batch. 

Add a line to set the batchsize. 


```
int batchSize = 128;
```


# Lab Step 5:

For Classification the number of nodes in the output layer is equal to the number of classes. 

Add this line to the code

```
int outputNum = 10;
```	 

# Lab Step 6:

An epoch is a total pass through the training data. Set numEpochs to 15

Add this line to the code

```
 int numEpochs = 15;
 ```	 

# Lab Step 7:

Add these two lines to define the paths to the train folder and the test folder

```
File trainData = new ClassPathResource("mnist_png/training").getFile();
File testData = new ClassPathResource("mnist_png/testing").getFile();
```

The needed imports for this section are.

```
import java.io.File;
import java.util.Random;
```

# Lab Step 8:

Define FileSplit for test and train by providing from File Path,Allowed Formats, and Random Number to FileSplit

```
FileSplit train = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS,randNumGen);
        FileSplit test = new FileSplit(testData,NativeImageLoader.ALLOWED_FORMATS,randNumGen);
```

The needed imports for this step are:

```
import org.datavec.api.split.FileSplit;
import org.datavec.api.util.ClassPathResource;
```


# Lab Step 9:

Create a Parent Path Label Generator to take the directory name as the label for the image. 

```
ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator();
```

The import required for this step is:

```
import org.datavec.api.io.labels.ParentPathLabelGenerator;
```

# Lab Step 10:

We will need two RecordReaders, one for the training data and one for the test data. 

Create the RecordReaders and proved height, width, channels, and LabelMaker.



```
ImageRecordReader recordReader = new ImageRecordReader(height,width,channels,labelMaker);
ImageRecordReader recordReaderTest = new ImageRecordReader(height,width,channels,labelMaker);
		
```

The import required for this step is:

```
import org.datavec.image.recordreader.ImageRecordReader;
```


# Lab Step 11:

Initialize the recordreaders. 

```
recordReader.initialize(train);
recordReaderTest.initialize(test);
```

# Lab Step 12:

A DataSetIterator builds an INDarray from the List of Writables that the RecordReader provides. 

Create two iterators one for the test data and one for training data. 


```
DataSetIterator dataIter = new RecordReaderDataSetIterator(recordReader,batchSize,1,outputNum);
DataSetIterator testIter = new RecordReaderDataSetIterator(recordReaderTest,batchSize,1,outputNum);
```		

The imports required for this step are:

```
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
```


# Lab Step 13:

Scale the pixel values between 0 and 1 with an ImagePreProcessingScaler.

Scaling the data will give better results. 


```
DataNormalization scaler = new ImagePreProcessingScaler(0,1);
```


The imports required for this step are:

```
import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;
import org.nd4j.linalg.dataset.api.preprocessor.ImagePreProcessingScaler;
```


# Lab Step 14:

Call the fit method on the scaler.

```
scaler.fit(dataIter);
```



# Lab Step 15:

Apply the scaler to the training iterator and the testing iterator using the setPreProcessor method of the datasetiterator.
        

```		
dataIter.setPreProcessor(scaler);
testIter.setPreProcessor(scaler);
```
# Lab Step 16:

Build the Neural Network.

The code for the Neural Network is available as a stub. Uncomment and examine the code. 

The imports required for this section *** steps 16,17,18,19 *** are:

```
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.inputs.InputType;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.weights.WeightInit;
import org.nd4j.linalg.activations.Activation;
```


This first section defines parameters applicable to all layers of the network. 

The Stub is complete except for 3 important settings, in this step you will set the 
* OptimizationAlgorithm
* learningRate
* Updater

```
 MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(rngseed)
            .optimizationAlgo(OptimizationAlgorithm.### YOUR CODE HERE ###)
            .iterations(1)
            .learningRate(###YOUR_CODE_HERE)
            .updater(Updater.###YOUR CODE HERE###).momentum(0.9)
            .regularization(true).l2(1e-4)
            .list()
```


Set the OptimizationAlgorithm to STOCHASTIC_GRADIENT_DESCENT

Set the learning Rate to 0.006

Set the Updater to Nesterovs.



# Lab Step 17:

The next section of the code uncommented in the above step defines a hidden layer. You need to add three parts, defining the nIn, the Activation, and the Initial Weights. 

```
.layer(0, new DenseLayer.Builder()
    .nIn(###YOUR CODE HERE####)
    .nOut(100)
    .activation(Activation.#### YOUR CODE HERE ####)
    .weightInit(WeightInit.#### YOUR CODE HERE ####)
    .build())
```

Set the nIn to "height * width", the height and width of the input images. 

Set the Activation for this layer to "RELU". 

Set the WeightInit to "XAVIER".


# Lab Step 18:

Complete the code for the output layer.




```
.layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
    .nIn(### YOUR CODE HERE ###)
    .nOut(### YOUR CODE HERE ###)
    .activation(Activation.### YOUR CODE HERE ###)
    .weightInit(WeightInit.### YOUR CODE HERE ###)
    .build())
```

Set the nIn for this layer to match the nOut for the previous layer. 

This Network is performing classification, set the nOut to equal the number of classes. In this case the number of digits, 0-9, "10". 

To convert the output of each output node to a probability per class, set the activation to "SOFTMAX"

Set the weight initialization algorithm to "XAVIER"


# Lab Step 19:

Complete the final block of code defining the configuration of the Neural Network


```
.pretrain(### YOUR CODE HERE ###).backprop(### YOUR CODE HERE ###)
    .setInputType(InputType.convolutional(height,width,channels))
    .build();

```

Pretrain applies to specialized neural networks that do unsupervised learning, it does not apply to this simple feed forward network. 

Set pretrain to "false"

Backprop is how the neural network trains, it should be set to true. 

Set backprop to "true"




# Lab Step 20:

Uncomment the rest of the code by removing the lines with ``` <-- remove this line -->```.

You will need the following imports:

```
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.ui.api.UIServer;
import org.deeplearning4j.ui.stats.StatsListener;
import org.deeplearning4j.ui.storage.InMemoryStatsStorage;
```



# Lab Step 21: 

View the Web Based User Interface as the network trains by opening a browser 

http://localhost:9000/





<!-- 

![alt text](../resources/venn.png)



---------
<div style="page-break-after: always;"></div>

-->
