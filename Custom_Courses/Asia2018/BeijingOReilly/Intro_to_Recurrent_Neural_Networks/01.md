!SLIDE center subsection

# 目录

* 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* RNN, LSTM的优势
* 建模序列
* 沿时间反向传播
* 调优指南

!SLIDE

# 目录

* **&rArr;** 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* RNN, LSTM的优势
* 建模序列
* 沿时间反向传播
* 调优指南


!SLIDE

# 循环神经网络(RNN)概述

* 带隐藏状态的前馈网络
* 拥有内部动态的隐藏状态
* 信息可以存储在“隐藏状态”中很长一段时间

!SLIDE

# 长短期记忆网络(LSTM)

* 将神经网络的动态状态看作是短期的
* 我们想要使这一过程持续很长时间（从原本RNN改进)
* 创建模块以便信息可以
  * 输入门控
  * 输出门控
  * 遗忘门控
* 在门控之间是关闭的，信息是被保存的
![LSTM](../resources/lstm.png)

!SLIDE



# 目录

* 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* **&rArr;** RNN, LSTM的优势
* 建模序列
* 沿时间反向传播
* 调优指南


!SLIDE

# RNN的优势

* 分布式隐藏状态
* 多个隐藏单元可以同时激活
  * 可以“记住”几种不同的东西
* 非线性
  * 允许以复杂的方式更新隐藏状态

!SLIDE

# RNN, LSTM 比前馈网络的优势

* 前馈网络存在梯度消失的问题使训练深度网络更有挑战性
* 线性反向传播
  * 梯度消失：接近零的值的复合乘法趋向于消失
  * 梯度爆炸：大值的复合乘法趋于爆炸
* 正向传播是非线性的
  * 激活函数（挤压函数）可防止向量爆炸
* 解决方案是LSTM以从反向传播中分离的方式保存时间信息-BPTT(backpropagation through time)



!SLIDE

# 基于固定时间窗, RNN比起前馈网络的好处

* 前馈网络-预先配置的时间步骤窗口
  * 硬编码
  * 需要关于特征的领域知识
* RNN-给予事件的灵活状态信息
  * 能够学习长时间灵活的事件
  * 在灵活的时间窗口中学习特征依赖关系

!SLIDE


# RNN比起前馈网络的另一个好处

* 前馈网络
  * 一对一的关系输入到输出
* RNN
  * 一对多的关系
	  * 一个图像到多单词标题
  * 多对多的关系
	  * 英语翻译成法语
  * 多对一的关系
	  * 语音分类

!SLIDE



# RNN 架构

![alt text](../resources/RNN_architectures.png)

* 固定大小的输入到固定大小的输出: 图象分类
* 序列输出: 图像描述，输入一个图像，输出一个句子
* 序列输入: 情绪分析，一个句子被分类为表达积极或消极情绪
* 序列输入与序列输出: 机器翻译，RNN读英语句子，然后输出法语句子
* 同步序列输入和输出: 视频分类，给视频的每一帧标上标签 ``

!SLIDE

# LSTM的成功案例

* 异常检测
* 手写识别
* 语音识别
* 图像字幕

!SLIDE

# 手写识别

* 输入是文本写入时的笔坐标序列
* 输出是字符序列
  * Graves & Schmidhuber (2009)
* 如果样本不是实时序列，小图像样本也可做为输入

!SLIDE

# 训练自然语言处理的数据需求

* RNN比其他网络需要更少的训练数据

!SLIDE

# 目录

* 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* RNN, LSTM的优势
* **&rArr;** 建模序列
* 沿时间反向传播
* 调优指南

!SLIDE

# 时间序列数据与循环神经网络

* 处理顺序或时间序列数据时
	* 倾向应用循环神经网络
* 允许我们输入随时间变化的数据
	* 定期收集的患者数据
	* 随着时间电网状态
	* 客户行为的序列

!SLIDE

# 序列数据与循环神经网络

* RNN具有识别数据里时间序列的能力
* 将文本语料库分解为一系列单个字符可使网络学习依赖关系，像是“Q”后最常见的字母是“U”，当一个引语被打开时它最终需被关闭
* 在实验时间，将训练LSTM来编写天气预报


!SLIDE

# RNN和前馈神经网络之间的差异

* RNN 允许随着时间的推移对向量建模的变化
* RNN 输入：多组特征向量
* FFN 输入：单个特征向量


!SLIDE

# 建模序列

* 从输入序列到输出序列
  * 法文到西班牙文
  * 语音识别
* 训练序列尝试预测当前+1步骤的值

!SLIDE

# 训练目标，一个序列到另一个序列

* 在对顺序数据进行建模时，我们通常要将一个序列转换为另一个序列

* 同个短语从法文到西班牙文

* 音频广告的序列转换成文字

!SLIDE

# 监督与非监督

* 对下一时间步进行预测的训练，模糊了监督与非监督之间的界限。

!SLIDE

# 案例研究

<img src="../resources/lstmsample1.png" align="center" height="580" width="820" >

!SLIDE

# 案例研究

<img src="../resources/lstmsample2.png" align="center" height="580" width="820" >


!SLIDE


# 使用LSTM的长期内存模式的使用案例

* 字符序列
  * 括号，引号，括号打开或关闭
  * 句子开头的大写
* 振荡
  * 正常
  * 异常
* 网络活动模式
  * 输入包和输出包流
  * 模式中的异常活动
* 金融交易序列
  * 正常
  * 异常


!SLIDE

# 目录

* 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* RNN, LSTM的优势
* 建模序列
* **&rArr;** 沿时间反向传播
* 调优指南

!SLIDE


# LSTM更新器:  沿时间反向传播 BPTT(Backpropagation Through Time)

* 照常计算梯度
* 修改以满足时间约束
* 向前传递在每个时间段构建活动堆栈
* 反向将活动从堆栈中取出活动并计算出错误
  * 这就是为什么称为沿时间反向传播的原因


!SLIDE

# 目录

* 循环神经网络(RNN), 长短期记忆网络(LSTM)概述
* RNN, LSTM的优势
* 建模序列
* 沿时间反向传播
* **&rArr;** 调优指南

!SLIDE

# LSTM 超参数调优

* 过拟合
  * 对训练数据有出色表现
  * 对测试数据表现差
* 使用正规化
  * l1
  * l2
  * dropout
* 大的网络较可能过度拟合
  * 避免从10,000个例子中学习一百万个参数
  * 参数 > 数据量 = 问题
  * 更多的数据总是更好的
